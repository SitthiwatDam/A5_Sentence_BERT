{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5: Sentence Embedding with BERT\n",
    "Name: Sitthiwat Damrongpreechar <br>\n",
    "Student ID: st123994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the link for dataset: https://huggingface.co/datasets/d0rj/wikisum. The dataset is involve with the wikipedia summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"d0rj/wikisum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.wikihow.com/Store-Fresh-Oysters',\n",
       " 'title': 'How to Store Fresh Oysters',\n",
       " 'summary': \"To store fresh oysters, start by placing un-shucked oysters on top of a layer of ice in a bowl. Next, lay a damp towel on top of the bowl and place it in your refrigerator set to 35° to 40° Fahrenheit. Make sure that the towel stays damp and replace the ice as needed. Then, shuck and eat the oysters within 2 days. If your oysters are already shucked or you need to store them for more than 2 days, place them in the freezer until you're ready to use them.\",\n",
       " 'article': \"Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40\\xa0°F (2 and 4\\xa0°C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5\\xa0in (1.3\\xa0cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad. \",\n",
       " 'step_headers': 'Do not shuck or wash your oysters. Pour ice into a small bowl or other open-top container. Place your oysters on top of the ice bed deep side down. Dampen a towel with cold water and place it on top of the oysters. Place your container in a refrigerator. Keep your oysters in the fridge for up to 2 days. Shuck the oysters when you’re ready to eat them. Keep the oysters in their shells and rinse them off. Place your oysters in a freezer-safe container. Pour oyster liquor into the container if you’re freezing shucked oysters. Seal the container. Keep your oysters in the freezer for up to 3 months. Thaw your oysters in the fridge before consuming. '}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40\\xa0°F (2 and 4\\xa0°C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5\\xa0in (1.3\\xa0cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad. \",\n",
       " 'Listen for the telltale \"tick, tick, tick\" rather than a much more rapid ticking noise. On standard watches, the motion of the second hand is jerky and truncated because the majority of them are quartz watches. The second hand shifts abruptly from each second position to the next. If you listen carefully, you can usually hear a quiet \"tick, tick, tick\" from this motion. On the other hand, Rolexes (and many other fine watches) have second hands that move almost perfectly smoothly because they have automatic movements not quartz. Because of this, Rolex does not make a \"ticking\" noise. If you hear a slow ticking noise coming from your watch, this is a dead giveaway that you\\'re not wearing a real Rolex. The noise you hear should be much faster than a battery operated watch. Look for jerky second hand motion. As noted above, Rolexes have second hands that smoothly sweep across the face of the watch, rather than jerking from one position to the next. Look at your watch\\'s second hand carefully — does it turn smoothly, tracing the path of a perfect circle around the edge of the watch\\'s face? Or does it appear to speed up, slow down, or jerk as it turns? If the second hand\\'s motion is anything less than silky smooth, you may have an imitation on your hands. In fact, if you look extremely closely, a real Rolex\\'s second hand motion isn\\'t perfectly smooth. Many models actually move at a speed of about 8 tiny movements per second. Some models even have slower speeds. To the naked eye, however, this motion is usually undetectable, so the second hand looks like it\\'s moving smoothly. Look for fake \"magnification\" of the date. Many (but not all) Rolex watches have a small dial or window that displays the date. Usually, this is on the right side of the watch face (near the \"three o\\'clock\" position). To make this dial easier to read, some Rolexes include a small magnification lens (sometimes called a \"cyclops\") in the glass over the dial. This part is difficult to counterfeit, so many fake Rolexes will have something that appears to be a magnification panel, but, on close inspection, is actually only ordinary glass. If the magnification panel over the date dial doesn\\'t actually seem to make the date numbering any larger, you may have a fake. Actual Rolex magnification windows should magnify the date to 2.5x — the date should take up nearly the entire window. Some good counterfeits will magnify the date somewhat but often not to the point that the entire window is filled. They will also not be centered over the date exactly. Be suspicious of a magnification window that looks glued on imperfectly or off center. Loosen the stem and roll back the hands to change the date, it should change to the previous date when it goes down to the 6 position, not at the 12. This is virtually impossible to replicate. If it does not do this it is likely a fake. Feel for a suspiciously light weight. Real Rolexes are constructed from real metal and crystal and thus have some heft to them. They should feel solid and substantial in your hand and on your wrist. If your Rolex feels suspiciously lightweight, it may not be of the highest quality — it may be lacking some of the precious metals used in many models of Rolex or may be constructed entirely from substandard materials. Look for a clear backside to the watch. Some imitation Rolexes feature a clear glass back which allows you to see the inner workings of the watch. This clear backing may or may not be concealed beneath a removable metal cover. In fact, no current models of Rolex contain this sort of clear caseback, so if your watch has this feature, it is not a true Rolex. Only a few Rolexes have ever been made with clear case backings, and these were all exhibition models. It is thought that counterfeiters add this clear caseback to help vendors sell watches to unwitting customers by allowing them to view the workmanship inside the watch. Inexperienced customers may be wowed by the inner workings of the watch, rather than alerted to the fact that something is wrong. Look for non-metal construction. Take your Rolex and turn it over. Examine the back of your watch — it should be made of smooth, unmarked, high-quality metal. If the band is not made of leather, it should be made from high-quality metal construction as well. If any part of the watch\\'s construction is made from plastic or a thin, cheap-looking metal like aluminum, you\\'re dealing with a fake. These qualities are clear signs that corners were cut during the manufacture of the watch. Rolexes are made from only the finest materials. No expense is spared in the creation of each watch. In addition, if the back casing of your watch appears to be made from metal but can be removed to reveal a plastic inner case, the watch isn\\'t genuine. Test the watch\\'s water-tightness. One surefire way to determine whether a supposed Rolex is real or not is to see if it is waterproof. All Rolex watches are made to be perfectly airtight — if your watch leaks even a little bit, it\\'s probably not the real thing. To test whether your watch is waterproof, fill a cup with water, make sure the stem is screwed on tightly and dunk the watch into the cup for several seconds, and take it out. The watch should be working perfectly fine and you shouldn\\'t see any water inside the dial. If you do, you have a fake on your hands. Obviously, if your watch is a fake, this test can harm or even ruin the watch. In the event of water damage, you may be forced to take the watch to an experienced repairman or even buy a new one entirely, so, if you\\'re not comfortable with these possibilities, try to rely on the other tests. Note that the Submariner is the only Rolex watch designed for deep water usage — while other Rolexes should be fine in the shower and the swimming pool, they may leak under more serious aquatic conditions. When all else fails, compare your watch to the real thing. If you\\'re still not sure if your watch is a real Rolex, it can be helpful to compare the way your watch looks to the way it is supposed to look. The Rolex website contains a catalog of all the watches Rolex produces, with multiple pictures for each. Find the model of watch you have on the Rolex site, then compare the appearance of your watch to that of the available \"reference\" images. Pay special attention to the dial — is everything laid out where it should be? If your watch has an extra dial like a chronograph or a date dial, is it in the right place? Are all the inscriptions identical? Is the lettering the same? If you can answer \"no\" to any of these questions, you probably have a fake. Rolex\\'s brand is famous for the quality of its craftsmanship — noticeable errors are extremely rare. Look for the serial number. Some expert-made counterfeits won\\'t be easy to tell apart from actual Rolexes. To spot these, you may need to examine the watch\\'s tiny, intricate detail work, which is the hardest part of the watch to fake. To start, try finding your watch\\'s serial number. This will require you to remove the band. You can usually do this by pushing the joint holding the band to the watch out of its place with a thumbtack or similarly-sized object. However, if you\\'re uncomfortable with this, you can also have a professional do it for you. The serial number should be located between the \"lugs\" at the six o\\'clock end of the dial. The lettering on the serial number should be perfect and precise, with fine lines. Some counterfeiters use an acid-etching method which produces serial number markings with a noticeable \"sandy\" appearance under magnification. Between the opposite set of lugs, there should be another similar marking. This is the case reference number and will be labeled with the words, \"ORIG ROLEX DESIGN.\" An original Rolex will have sharp and detailed engraving located between the lugs. Counterfeiters often try to mimic these engraving, the result often appears like the serial number has been roughly etched into the casing. Note that it\\'s possible to look the date of your watch\\'s manufacture up with your serial number — several handy online sources (like this one ) can help you here. Look for the crown at six o\\'clock. Starting in the early to mid 2000s, Rolex began etching the trademark crown logo into the crystal of their watch\\'s dials. If your watch was made in the past decade or so, you may be able to see this tiny mark of authenticity. Use a magnifying glass or a jeweler\\'s lens to carefully examine the glass at the six o\\'clock end of the watch\\'s dial. Look for the Rolex crown logo — the same design as the much larger logo at the opposite end of the dial. The etching you are looking for is very, very small and can be quite tricky to see. You may find that it is easier to see if you shine light at the face of the watch at an angle. Some counterfeiters do attempt to copy this etching, but it is extremely difficult to duplicate with the precision of an actual Rolex. If this etching is large enough to easily see with the naked eye, you may have a fake on your hands. Look for etched inscription inside the rim of the dial. Another mark of authenticity is the fine, etched lettering that is usually included around the rim of Rolex watch dials. Examine this lettering with a magnifying glass or jeweler\\'s lens. The lettering should be fine, precise, and elegant, with no imperfections. Additionally, the lettering should be etched into the metal rim. If it appears to be painted or printed instead, the watch is likely a fake. Note that, typically, all watches from Rolex\\'s Oyster series bear this etching. Watches from the Cellini series often have non-standard designs (rectangular faces, etc.) and thus may not have this etching. Look for a high-quality crown logo on the dial. Almost (though not quite ) all Rolex watches have the trademark crown logo located at the top of the dial near the twelve o\\'clock marking. Examining this logo under magnification can sometimes reveal a fake. The logo should appear to made of high-quality metal construction. The circles at the end of the crown\\'s points should have raised bumps. The outline of the crown should glimmer with a different metallic sheen than the inside. If your crown logo looks cheap or flat under magnification, this is a sign of poor craftsmanship (and a likely indicator of a fake). Look for perfectly precise lettering on the dial. Rolexes are renowned for their perfection. Even small, relatively undetectable defects can be clues that your Rolex isn\\'t top quality. Examine the lettering on your watch\\'s dial with a magnifying glass or jeweler\\'s lens. Each letter should be perfectly, precisely formed with straight lines and smooth curves. The spaces between words and letters should be consistent. If you notice that any letters seem to be even slightly uneven or smudged under magnification, this is a sign that the watch was made with sub-optimal printing technology and is probably not a Rolex. It\\'s also worth mentioning that, obviously, any sort of misspelling is also a dead giveaway that the watch is a fake. Beware sub-par packaging. Everything about a Rolex watch should be elegant, dignified, and perfect. This even includes the packaging. Real Rolexes come in fine jewelry boxes which usually include a mount to hold and display the watch as well as a small cloth to clean and polish it. All packaging should bear the official Rolex name and logo. The watch should also come with a manual and warranty paperwork. If your watch is missing any of these things, it may not be genuine. Buying a watch by itself off the street is a complete crap shoot — since there\\'s no packaging, there\\'s no way to tell it\\'s authentic. Beware shady locations. When shopping for Rolexes, use common sense. A reputable jeweler or a fine watch dealer is much, much more likely to be selling genuine Rolexes than a street vendor. Rolexes can cost thousands of dollars, so it\\'s safe to assume that anyone who sells them will have the resources to own a legitimate business. If you\\'re unsure whether a certain retailer is a reputable Rolex seller, consult Rolex\\'s online listing of certified retailers here. Pawn shops can be a mixed bag — they may have genuine Rolexes, but they may not, depending on the individuals who sold the shop the watches. Some pawn shops take efforts to ensure that they only sell genuine watches, while others may turn a blind eye to counterfeits. If you don\\'t know whether a certain pawn shop can be trusted, try to find online reviews and testimonials for the store before making your purchase. Beware unusually cheap prices. When it comes to buying Rolexes, if a deal seems too good to be true, it probably is. Rolex watches are fine-made luxury goods crafted to perfection — they\\'re never cheap. The most expensive Rolex watches in the world sell for over a million dollars, while even some of the cheapest models can sell for over $4000. If you\\'re being offered a Rolex for $100 dollars, it doesn\\'t matter what the seller\\'s explanation is — there\\'s either something wrong with the watch or it\\'s not the real thing. Don\\'t accept an unscrupulous seller\\'s excuses. If you\\'re being told that a Rolex watch is being sold for cheap because the seller found it or because it was given as a gift, walk away. Assume that there are no lucky coincidences when spending the kind of money that it takes to purchase a Rolex. When all else fails, take your watch to an experienced jeweler. Sometimes, even when you know what to look for, it\\'s almost impossible to tell whether a watch is the real deal or a fake. In these cases, a knowledgeable, trustworthy jeweler or watch salesman can help you by examining the watch for qualities that the ordinary person can\\'t catch. If you have a great relationship with this expert, you may be able to have your watch\\'s genuineness judged for free. Otherwise, jewelry appraisal services, while not cheap, are fairly affordable when compared to the price of a Rolex. For instance, some jewelry appraisal services can run at a cost of up to $180 per hour. Because of this, you may want to have multiple items appraised at once to get the greatest possible value. Only use jewelry appraisal services that charge by the hour, on a per-piece basis, or for a contracted amount based on the estimated time needed. Never use appraisers that charge a percentage of the jewelry\\'s value — this is a scamming technique. Finished.  ',\n",
       " \"Use an Allen key to unscrew the handlebars from the bike. Put the Allen key into the screw on the front of your handlebars. Turn the wrench counterclockwise to loosen the screw. Once it's loose, you should be able to lift your handlebars from their housing. Place the screws and bolts in a zip lock bag to keep the smaller parts together for when you want to reassemble the bike later. Remove the pedals. Depending on what kind of pedals you have, you'll need either an Allen key or a 15mm wrench. Press down on the pedal with your foot and hold the brake to prevent your bike and pedals from moving. Bend down and insert the Allen key or use the wrench to loosen the bolt on the opposite side of the pedal. Continue to unscrew the bolt until the pedal comes off. You may need a friend to help you hold the bike in place while you unscrew the pedals. Remove the front wheel. If you have a quick-release wheel, all you'll need to do is undo the latch on the front of the wheel and turn the latch counterclockwise to loosen the wheel. If your wheel is secured with a bolt, you'll need a 15mm wrench to remove it. Loosen the bolt on the center of the wheel by turning the wrench counter-clockwise. You may have to loosen the brakes to remove the wheel. Unscrew and take off your seat. Find the bolts that are holding your seat to the frame of the bike and remove them with an Allen key. These bolts will be somewhere under the seat. Some bikes will have two bolts that you need to undo while others will just have one. Turn the key counterclockwise to loosen the bolt until the seat comes off. Your bike should now be mostly unassembled and ready for transport. Let the air out of your tires by pressing the needle on the nozzle. Deflate your tires to make more room in your shipping container for the rest of the bike. Doing this will also prevent the tubes from popping while you're shipping it. Unscrew the caps on your bike's tires and softly press down on the needle in the center of the valve to release the air from the tires. Don't squeeze your tires before you depress the needle or you may damage them. Get a cardboard bike box at a bike shop. Find a box made for the same style of bike that you have. See if you can try to get the box for free. If the bike shop doesn't have any boxes, you can purchase one online for a low fee. Purchase a bike travel case for more expensive bikes. Bike travel cases are more expensive than a cardboard box but provide additional protection for your bike. If you are shipping a more expensive bike, consider using a travel case. This may save you money in the long run because you won't have to pay for costly bike repairs. Purchase and cut plumbing insulation to the size of your frame. Purchase pipe insulation online or at a hardware store. This foam material can wrap around the frame of your bike and prevent it from getting damaged during transport. Measure the length on the different parts of your bike's frame and cut the foam insulation to the same size. If you're trying to save even more money, you can use bubble wrap instead of plumbing insulation. Tape or tie the insulation around your bike frame. Wrap the insulation around the frame of your bike and wrap duct tape or zip ties around the insulation and frame. This will secure the foam insulation to your bike's frame and hold it in place. Wrap loose parts with bubble wrap. Take your wheel, handlebars, and pedals and wrap them with bubble wrap. Secure the wrap with tape and set them aside. You should also cover any remaining exposed parts of your bike with whatever bubble wrap you have left over. Place the frame of the bike into the container first. Place the bike's frame and back wheel into the top of the container. Double check to make sure that your bike box is large enough to accommodate the bike's size. If you have a heavy bike, you may need someone to help hold the box in place as you put the frame in it. Place the rest of the parts into the container. Carefully place the rest of the parts into the container, sliding them up as close to the frame as possible. Once you position all of the parts, close the box and make sure that the sides are not bulging. If the sides are bulging, see if you can rearrange the parts in the box so that it doesn't. If the frame is causing the sides of the box to bulge, you'll need a larger box. Tape the box closed. Close all the flaps on the box and apply several coats of tape over the flaps. Once you feel like the box is secure, you're done packing away your bike. Decide which shipping company you want to use. Compare the prices from different shipping companies so that you can find the cheapest option. Major shipping companies like FedEx, USPS, and UPS can ship your bike for a fee. There are also other services like Bikeflights, Sports Express, and Shipbikes.com that specifically ship bicycles. Measure the dimensions of your bike and determine how much each service would cost. Print out a shipping label. You can usually calculate the cost and print and pay for shipping online. Go to the website for the shipping company that you're going to use and fill out the order information. Tape your shipping label to the box securely. Make sure to go over the front of the label with a clear masking tape so that the label won't get damaged in transit. If you can't print the label online, you'll have to visit a shipping facility. Have the bike's destination on hand when filling out the shipping label. Some companies won't ship your box unless all over the edges of the shipping label are securely taped to the box. Take your bike to a shipping facility and pay the fee to send it. Take your package and bring it to the closest shipping facility. If you weren't able to get a label online, you can now get one at the facility. They will weigh and measure the package for you and then tell you how much it will cost. Shipping a bike that weighs 50 pounds (23\\xa0kg) can cost anywhere from $100-$400. Keep your shipping container as small as possible. Some shipping companies will charge you based on the size of your box, not the weight of your bike. If your shipping company is charging by box size, choose the smallest box you can find. Use basic shipping. Most of the time shipping companies will have a slower ground option that you can choose. Choosing this option will usually take longer but will cost you less. Take your bike on an airline flight. Check the regulations and fees associated with traveling with your bike. Some airlines offer to transport your bike in the plane. This prevents the likelihood the bike will get lost and can sometimes cost less than shipping it. Consider insuring your bike if it's expensive. While the price of insurance will raise your overall cost, it will prevent you from having to buy a new one if it's lost in transit. This is especially useful if you're shipping the bike internationally. \",\n",
       " \"Choose a water-based sealer if your pavers are laid with joint sand. Water-based sealers are sprayed on and give an even coverage. The main benefit of a water-based sealer is that it hardens the joint sand and penetrates into pavers better. This ends up creating a hard seal over the entire paved area. Water-based sealers do not darken the stones or give a high amount of shine to the pavers. Choose this sealer for bricks that are porous and won't shine much anyway. If you are unsure what type of pavers you have or you aren't sure which sealer is best, consult a home improvement store employee. Water-based sealers often require that you apply more than one coat to the paved area. Buy a solvent sealer to darken stones and make them shine. Solvent sealers can give a much higher gloss to the stones than water-based sealers. The downside is that they do not harden sand as effectively. This sealer is great for smooth, non-porous stones. If your pavers are laid tightly together without joint sand, this is a good option. Solvent sealers are often thicker than water-based sealers, so you may be required to use a roller rather than a sprayer. Read the instructions for proper application. Sealers may be sprayed on with a pump sprayer, laid with a nap roller, or a combination of both. Follow the instructions on the sealer you buy to use the proper tools. Thin sealers can often be sprayed on, which is much faster. Thicker sealers may need to be rolled, which can be slower and won't always cover as evenly. Some sealers are sprayed on but require that you backroll the pavers to even out the coating. Use a sprayer with an adjustable nozzle head. For best application, you want a nice wide fan spray, so be sure that the sprayer you use has either multiple head attachments or an adjustable spray pattern. Never use a sprayer that shoots a narrow stream. A bug sprayer is a good option for this. They are widely available at hardware stores, and they are often inexpensive. Use a roller of appropriate thickness. Many sealers will require that you lay them with a roller rather than a sprayer, and in this case it is important that the roller is thick enough. The sealer will most likely give you instructions about this. For example, one sealer might suggest a roller with a 1” thick nap. This is important because if the roller is not thick enough, the sealer will not get down into the cracks between the pavers and thus won't seal as well as it should. A roller that is too thick might coat the sealer on too thickly as well, and you won't get an even coat across the pavers. Wash the pavers. Sealing without cleaning first is only going to seal dirt and grime into the pavers. You can power wash the pavers, as long as there isn't sand in between that would be washed away. You could use some dish soap and hot water for areas that have oil or tire marks from cars. A stiff broom will help scrub the pavers clean. Let the pavers dry for at least 24 hours before you continue. The type of pavers and the way they are laid will determine the best washing method. If the pavers are packed tightly together, power washing is a great option, but not if the pavers are laid with sand between them. It's rarely a good idea to sand-blast pavers. If the pavers make up a driveway that has a lot of car traffic, wash the pavers more thoroughly to get as much of the grime off as you can. Smooth pavers that don't retain much grime might be sufficiently cleaned with a quick broom sweeping. Level crooked pavers. Sometimes pavers settle unevenly, which creates a tripping hazard in areas that people frequently walk. Lift any uneven pavers and level the spot where they're sitting by adding or removing sand under the paver. If your pavers are laid tightly together, lifting any of them up to even them out may not be an option. Still, sealing is more effective when the pavers are all level. Seal the pavers one section at a time. To make sure you cover the whole paver area, work in a specific pattern. A good option can be going around the entire outer edge of the area first to establish a border, then dividing the rest of the area into 3-5 sections depending on how big it is. For example, a 100 square foot patio could be done in four 5ftx5ft sections. Start at a corner farthest away from your exit, and work your way out towards the exit. This will keep you from walking across the sealant at the end. Move the sprayer nozzle in a circular motion. The sealer will make stones appear darker when they get wet, but it can still be helpful to use a consistent motion. You can move the spraying nozzle in circles for the best coverage. You don't want to cover some areas with more sealer than others, so it is important to pay attention to what stones you cover as you spray. Try not to use too much sealer, as it will take longer to dry. A light layer is all you need. If you are working in a small area, you may not have a hard time keeping track of what you have sealed, but if you are doing an entire driveway it becomes more important. Finish the whole job at once. Sealing pavers is not the kind of thing you want to break up into multiple rounds of work. If you don't do the whole thing at one time, it's likely that you will lose track of where you have already sealed. The amount of time it takes you to seal the entire area will depend on how big it is. A small patio may only take 20 minutes, but an entire driveway will likely take you a couple of hours. Clean the sprayer or roller. Rinse the sprayer or roller thoroughly immediately after you finish the project. If you let the sealer sit for too long it will harden on your equipment. If you don't want to wash the roller, you can throw it away. Make sure to spray water through the nozzle until the water runs clear to clean the hose as well. When you read the sealer instructions, see if there are any special instructions about cleaning equipment. You may need lacquer thinner or something stronger than water to clean the sealer out of your sprayer. \",\n",
       " 'Remain calm. Although emergencies require rapid action, the most important factor in effectively handling the situation is to keep calm. If you find yourself becoming confused or anxious, stop what you\\'re doing. Take a deep breath. Remember that to be calm in a stressful situation you must deliberately adjust your behavior. Reassure yourself that you can handle the situation. The reason you feel panicked in an emergency is the result of your body\\'s automatic overproduction of the stress hormone cortisol. The cortisol goes to the brain and slows down the pre-frontal cortex, which is the region responsible for planning complex action. By overriding your body\\'s reaction, you can continue to access your critical thinking faculties. You won\\'t be responding from emotion, but from rational thought. Look around and assess the situation to see what needs to be done before acting. Seek additional help. In the USA, call 911 for emergency assistance. Use whatever number is applicable to call emergency services outside the U.S. This phone number will reach an emergency dispatcher who will need to know your location and the nature of the emergency. Answer all of the questions the dispatcher asks. The job of the dispatcher is to provide quick, appropriate emergency response. She can only do this by asking these questions. If you\\'re calling on a traditional telephone or a GPS-equipped cell phone, emergency services may be able to track your location even if you\\'re unable to speak. Even if you can\\'t talk, call emergency services and someone will be able to find you to provide help. It may be a good idea to go over how you would communicate during an emergency , especially if you have a reason to expect an emergency might arise. Determine the nature of the emergency. What signs indicate that there is an emergency? Is this a medical emergency, or is there a threat to the property/building that may result in human injury? It\\'s important to stop and take inventory of the situation calmly before reacting to the emergency. An injury due to motor vehicle accident, or smoke inhalation or burns from a fire are examples of medical emergency situations. A medical emergency consists of sudden physical symptoms, such as intense bleeding, head trauma, loss of consciousness, chest pain, choking, sudden dizziness or weakness. Intense desire to hurt yourself or someone else constitute a mental health emergency. Other mental health changes may also be considered an emergency, such as sudden changes in behavior or experiencing confusion, can be an emergency if they occur without cause. Behavioral emergencies are best met by remaining calm, watching from a short distance, and encouraging the person in crisis to stay calm as well. This way you can react appropriately if the situation becomes volatile. Know that sudden changes can be emergencies. Chemical spills, fires, breaking water pipes, electrical outages, natural disasters such as floods or fires are all examples of potential workplace emergencies. If you have advance warning of the possibility of an emergency, such the warning of flood, heavy snow, tornado, etc., you may be better prepared. However, the nature of an emergency is to be unexpected. When assessing emergency situations, be aware that the situation may be volatile. It may change rapidly. If you have advance warning of an emergency, prepare ahead of time for the best results. Be alert for human-caused emergencies. Assaults or threats of violence at a workplace or home are emergencies that call for rapid response. In most cases, there is no predictable pattern or method to these emergencies. These situations tend to be unpredictable, and they change quickly. If you find yourself in an emergency of this nature, keep yourself safe. Run to a safe location, or find shelter in place. Do not fight, except as a last resort. Being attentive to warning signs in your workplace, including any act of physical violence (pushing, shoving, etc.) should be immediately. Your office should have a procedure for workplace violence, including a phone number you can call to report the situation. If you don\\'t know your office\\'s procedures, ask your supervisor or a trusted coworker. Open, honest communication between employees and supervisors is part of maintaining a safe, healthy workplace. Assess the immediate threat. For example, if one person appears injured, are you or anyone else in danger of also being injured? For example, if one person is caught in a machine, is the machine turned off? If there has been a chemical spill, is the spill spreading towards anyone else? Is the person caught in structure that\\'s collapsing? If the threat isn\\'t contained, this will affect your response. Be aware that any emergency situation may change abruptly, so that ongoing assessment is required. Remove yourself from danger. If you, or others, are at risk of being harmed, leave the situation immediately. If you have an evacuation plan, follow it. Go to an area where you will be safe. In a situation where you cannot leave, find the safest possible location within your given area. For example, hiding beneath a solid surface, such as a desk or table, may help if there is a chance of being hit by falling debris. If you\\'re near a car accident, make sure you\\'re not in the line of oncoming traffic. Get off the road. Be aware that in an emergency, elements are likely to change quickly. In your assessment, notice if volatile or combustible elements are present. For example, in an auto accident, gasoline may catch fire abruptly. Help others leave a hazardous area. If you can safely assist someone else in leaving a dangerous situation, do so. If returning to the emergency situation is risky, a trained rescue person may be better equipped to retrieve anyone in harm\\'s way. Offering verbal reassurance to an injured person if he is conscious will help another person, even if you can\\'t move them. Let that person know who you are and what is happening to them. Ask them questions to keep them conscious. If the emergency is stable, stay with the victim. Determine if you can do anything to help. The most important thing you can do is to remain calm, and stay in control of the situation. Sometimes there is nothing that you can do, and that\\'s fine. Don\\'t be worried about admitting that there\\'s nothing you can do to help. If there are others on the scene who may be upset or fearful, reassure them. Employ them in going to get help. It\\'s better to remain with someone in a supportive way than to do an action that may result in additional damage. If you\\'re not sure what to do, simply stay with the person. If possible, take their pulse, make notes of events as they happen, and ask them about their medical history. This is information you may need when talking to the emergency team. Take time to think before acting. Being in an emergency situation can result in panicked thinking and actions. Instead of reacting to a situation, take time to calm down. Breathe deeply before you take any action. Things change suddenly in emergency situations. Don\\'t panic if things suddenly go in a different direction than you\\'d expected. Take time to pause whenever you\\'re overwhelmed, panicky or confused. If you need to stop in the middle of taking an action to calm down, that\\'s okay. Get the first aid kit. A first aid kit should have constructive tools for taking care of many medical emergencies. Any first aid kit should contain bandages, gauze, adhesive tape, disinfectant, and other useful items. If you can\\'t retrieve the first aid kit, consider what other items in your immediate vicinity might be good substitutes. You should keep a first aid kit at your home, and your workplace is required by law to maintain a first aid kit. A good first aid kit should also have a \"space blanket\" which is a light-weight piece of special material meant to conserve body heat. This is a vital piece of equipment for people who are chilled or shaking, as it can help keep them from going into shock. Ask basic questions of the injured person. It\\'s important to discern the mental state of the victim in order to better understand the person\\'s injuries. If the person appears confused by the question, or provides the wrong answer, this may suggest additional injuries. If you\\'re not sure if the victim is unconscious, touch their shoulder. Shout or ask loudly, \"Are you okay?\" Questions you should ask include: What is your name? What is the date? How old are you? If they do not respond to questions, you can try rubbing their chest or pinching their earlobe to keep them conscious. You can also gently touch the eyelids to see if they will open. Once you\\'ve determined the basic mental status of the person, check with them about any medical complications. Ask them if they have a medical alert bracelet or another medical ID. Avoid moving an injured person. If someone has a neck injury, moving him could result in injuring the spine. Always call emergency services if someone has an neck injury and is unable to move himself. If the person can\\'t walk because of leg or foot injuries, you can help move them by holding them at the shoulders. If the person is afraid to leave a dangerous situation, respond with reassurance. Use the telephone only to ask for help. Your full attention should be on the present situation, and talking on the phone is distracting. In addition, if you are on an older model telephone, the emergency dispatcher may be trying to reach you. Stay off the phone unless you\\'re calling to ask for help. If you\\'re not sure if you\\'re in a true emergency, call emergency services and the dispatcher can help you figure out if emergency officials should be sent. Don\\'t try to document the emergency unless you are sure you are out of danger. Taking \"selfies\" or posting about your situation on social media in ongoing emergency situations may result in additional injury and legal complications. Have an emergency plan. The best response in an emergency situation is to follow the emergency plan of your home or workplace. Certain people may be identified as emergency leaders, with special training. In an emergency, you will save necessary time and energy by following the plan and your designated leader, even if you don\\'t fully agree with them. Your emergency plan should have an assembly place to gather once you\\'ve evacuated the home or building. Keep emergency phone numbers posted near the phone. Important medical data should be stored in your phone or your wallet. Know your physical address. You\\'ll need to know your location in order to tell any emergency dispatcher where to send help. While it may be easy to know the address of your home, it\\'s also important to memorize the address of your workplace. Get into the habit of checking the address wherever you are. If you don\\'t know the physical address, be ready to say the name of the street you\\'re on and any nearby intersections or landmarks. If your cell phone has GPS, you can use it to determine your physical address. However, this wastes much needed time in an emergency. Identify your closest exits. Always be aware of the exits to any building you\\'re in, whether they\\'re home, office, or commercial locations. Identify at least 2 exits, in case one is blocked. In a workplace or public location, exits should be clearly marked. Choose two places where you can regather with your family or coworkers. One location should be outside the home or workplace. The other location should be outside the immediate vicinity, in case the neighborhood is unsafe. Emergency exits should be physically accessible, according to ADA laws. Take a first aid course. Having a first aid kit isn\\'t helpful unless you have training to use it. Having training to properly apply bandages, compresses, tourniquets and other tools will help in an emergency. The Red Cross regularly offers these courses in most areas of the US. Many Red Cross courses are also offered online. First aid courses can be age specific. If you have children, or just want to know how to help children in case of an emergency, take a first aid course specific to assisting children in an emergency. If you work with children, you\\'ll be required by law to receive this training. Consider taking CPR in addition to first aid. Having CPR (cardiopulmonary resuscitation) training is a life-saving help for someone having a heart attack. If you haven\\'t taken a CPR course, you can still offer chest compressions for someone suspected of having a heart attack. A chest compression is hard pressure applied swiftly to the ribcage at the rate of 100 compressions per minute, or just over 1 per second. CPR for children and infants is taught by the Red Cross. If you have children, take a course in providing CPR for children in order to be prepared in case of an emergency. If you work with children, you may be required by law to receive this training. Know what chemicals are found in your home or workplace. If the emergency occurs in your workplace, you should know where to find the MSDS (Material Data Safety Sheet) for any chemical used. Having a list of the chemicals used in your home or workplace, together with any first aid measures required in case of emergency, will be the most effective way you can prepare for emergency situations. Your workplace should have an eyewash station if you regularly come into contact with hazardous chemicals. Make sure you are prepared to share any relevant information regarding chemicals with your emergency response team. Keep emergency phone numbers posted near the phone. Post the number for 911 as well as other important medical phone numbers, including the phone numbers of family members who should be contacted. The phone number of the poison control center, ambulance center, your doctors\\' phone numbers should be posted alongside contact numbers of neighbors or nearby friends or relatives, and work phone numbers. All members of your house, including your children, should be able to access these phone numbers in case of an emergency. For children, elderly people, or disabled people, consider having a posted script to help them remember what to tell others when calling on the phone with an emergency situation. You can even role play with them to go over the script and teach them proper actions for different emergency situations. Wear a medical ID tag if you have a chronic health condition. If you have a condition that a medical response team should be aware of, such as diabetes, certain allergies, epilepsy or other seizure disorder, or other medical conditions, a medical ID tag can provide this information should you be unable to. Most medical responders look on a person\\'s wrist for medical ID tags. The second most common place to look is at the person\\'s neck, as a necklace. People with disabilities and health conditions, such as Tourette syndrome, autism, dementia, etc., may wish to consider wearing medical ID badges to help any emergency responder better understand their needs and behavior. ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_texts = dataset['train'][:100]['article']\n",
    "print(len(raw_texts))\n",
    "raw_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Do not shuck or wash your oysters. Oysters taste best when you shuck them immediately before eating them. In addition, keeping oysters in their shells makes them easier to store and reduces the chance that they'll go bad. If your oysters came pre-shucked in a plastic container, store them in the freezer until you're ready to use them. Leave the grit and dirt on the oysters. This will keep them moist and will help to insulate the meat. Pour ice into a small bowl or other open-top container. Grab a bowl, small cooler, or similar container that you can place inside your fridge. Make sure this container has an open top or removable lid. Then, pour a layer of ice into the bottom of the container. Do not keep your oysters in a sealed or closed-top container. Doing so will suffocate them. You may need to change your ice during the refrigeration process, so do not pour any into the container if you won't be able to check your oysters regularly. Place your oysters on top of the ice bed deep side down. Just like seafood merchants, you'll be storing your oysters on ice to keep them as chilled and fresh as possible. Make sure to turn each of your oysters so that the deeper side faces down, a technique that will help them better retain their juices. Dampen a towel with cold water and place it on top of the oysters. Dip a thin, clean kitchen towel in cold water and ring out the excess liquid. Then, gently lay the towel on top of the oysters. This will keep the oysters from drying out while preventing fresh water poisoning. If you'd prefer, you can cover the oysters with damp paper towels or newspaper instead. Oysters are salt water creatures, so submerging them in fresh water will essentially poison them and lead to their death. Place your container in a refrigerator. If possible, set your refrigerator to a temperature between 35 and 40\\xa0°F (2 and 4\\xa0°C). Make sure to store your oysters above any raw meat so the juices don't drip down onto your shellfish. If possible, check on your oysters at least once a day while they're in the fridge. If the towel dries out, dampen it again. If the ice in your container melts, pour it out and replace it with new ice. Keep your oysters in the fridge for up to 2 days. For safety, remove and consume your oysters within about 2 days of initially storing them. Though some oysters may last for a week or longer, eating them that late puts you at greater risk of food poisoning and other unwanted ailments. If your oysters came with an expiration date, use that as your guide for maximum storage time. Freeze your oysters if you need to store them for more than 2 days. Shuck the oysters when you’re ready to eat them. Once you finish storing the oysters, run them under cool water and open their shells. Then, run a knife under the flat side of the oyster and pop the shell off. Before eating, carefully separate the oyster from the rest of the shell using a knife. Before eating an oyster, inspect it to make sure it is still good. If the shell appears to be damaged, if the oyster smells foul, or if the meat is a cloudy shade of grey, brown, black, or pink, throw the oyster away. Keep the oysters in their shells and rinse them off. Storing your oysters inside their shells will make them less likely to go bad and, in some cases, better preserve their taste. Unlike refrigerating oysters, rinsing the shells under cold water to clean them off prevents any bacteria from living on the oysters. If you don't have enough room in your freezer to keep full-shelled oysters, you can shuck them before storage. If you do so, save the internal liquor for later use. Place your oysters in a freezer-safe container. To keep your oysters safe, place them inside a moisture-resistant, freezer-safe bag. If you're storing shucked oysters, you can use a firm plastic container instead. To prevent freezer burns, leave no more than 0.5\\xa0in (1.3\\xa0cm) of head space in the container. Pour oyster liquor into the container if you’re freezing shucked oysters. To help your shucked oysters retain their juiciness, pour the liquor you removed during the shucking process into your freezer-safe container. Keep pouring until you've completely submerged the oysters inside the liquid. If you don't have enough liquor to fill the container, pour in water as well. Seal the container. If you're using a resealable bag, press any excess air out of it using your fingers. Then, seal your container right before you put it into the freezer. Unlike with refrigerated oysters, closing the container will help better preserve your shellfish during long-term storage. If you're using a solid plastic container, make sure the lid you seal it with is air-tight. Make sure to write the initial storage date on your container. Keep your oysters in the freezer for up to 3 months. When frozen properly, fresh oysters should last for between 2 and 3 months. To make sure your oysters aren't going bad, look over them regularly and remove any that have cracked shells or cloudy meat that is a pink, black, brown, or grey color. While your oysters may remain safe to eat during this time, the taste will degrade gradually. Thaw your oysters in the fridge before consuming. Carefully take your oyster container out of the freezer and place it in a clear, open part of your refrigerator. Depending on the exact temperature of your appliances, the thawing process could take up to 20 hours to complete. Thawing your oysters using this method gives them a slightly longer shelf life, meaning you don't have to use them immediately after they thaw. If you'd like, you can thaw your oysters by submerging their container in cold water. However, you'll have to consume them immediately after they thaw, otherwise they'll go bad. \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_dataset = '\\n'.join(dataset['train'][:1]['article'])\n",
    "joined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(joined_dataset)\n",
    "sentences = list(doc.sents)\n",
    "if len(sentences)>10:\n",
    "    sentences = sentences[:10].copy()\n",
    "# lowercase the text\n",
    "text = [sent.text.lower() for sent in sentences]\n",
    "# Remove the symbols\n",
    "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Do not shuck or wash your oysters.] 10\n",
      "['do not shuck or wash your oysters'] 10\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:1],len(sentences))\n",
    "print(text[:1],len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Making Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine everything into one to make vocabs\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  #special tokens.\n",
    "\n",
    "#create the word2id\n",
    "for i, w in enumerate(word_list):\n",
    "    word2id[w] = i + 4  #because 0-3 are already occupied\n",
    "id2word = {i: w for i, w in enumerate(word2id)}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "#list of all tokens for whole text\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_list:  10\n",
      "word_list: ['your', 'preshucked', 'not', 'makes', 'wash'] 80\n",
      "vocab_size 84\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('token_list: ',len(token_list))\n",
    "print('word_list:',word_list[:5],len(word_list))\n",
    "print('vocab_size', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in word_list[:5]:\n",
    "    print(word2id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of these\n",
    "import pickle\n",
    "with open('./word2id.pickle','wb') as f:\n",
    "  pickle.dump(word2id, f)\n",
    "with open('./id2word.pickle','wb') as f:\n",
    "  pickle.dump(id2word, f)\n",
    "with open('./token_list.pickle','wb') as f:\n",
    "  pickle.dump(token_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n",
    "max_len    = 300 #maximum length that my transformer will accept.....all sentence will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "\n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3 masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]']\n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 300]),\n",
       " torch.Size([6, 300]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " tensor([1, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25, 33, 49, 48, 47],\n",
       "        [ 8, 39, 63,  7, 60],\n",
       "        [49, 18, 49, 24, 46],\n",
       "        [ 9, 54, 65, 76, 63],\n",
       "        [67, 48,  4, 81, 53],\n",
       "        [ 9, 80, 46, 76, 81]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "  \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "\n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp, h_pooled\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 60.859940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
    "bert_loss = []\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp, _ = model(input_ids, segment_ids, masked_pos)\n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "\n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    bert_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_loss = [i.detach().numpy() for i in bert_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(bert_loss, label = 'train loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BERT Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bert_path = './model/bert.pth'\n",
    "torch.save(model.state_dict(), save_bert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(84, 768)\n",
       "    (pos_embed): Embedding(300, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=84, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = BERT()\n",
    "loaded_model.load_state_dict(torch.load(save_bert_path))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_dataset = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 550152\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class in 'label'\n",
    "np.unique(snli_dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove class -1\n",
    "snli_dataset = snli_dataset.filter(lambda x: 0 if x['label'] == -1 else 1)\n",
    "# Recheck the class in 'label'\n",
    "np.unique(snli_dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 9824\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 549367\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the size of snli_dataset\n",
    "from datasets import DatasetDict\n",
    "snli_dataset = DatasetDict({\n",
    "    'train': snli_dataset['train'].shuffle(seed=SEED).select(list(range(4))),\n",
    "    'test': snli_dataset['test'].shuffle(seed=SEED).select(list(range(4))),\n",
    "    'validation': snli_dataset['validation'].shuffle(seed=SEED).select(list(range(4)))\n",
    "})\n",
    "snli_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessing function\n",
    "def preprocess(raw_texts):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(raw_texts)\n",
    "    sentences = list(doc.sents)\n",
    "    if len(sentences)>10:\n",
    "        sentences = sentences[:10].copy()\n",
    "    # lowercase the text\n",
    "    text = [sent.text.lower() for sent in sentences]\n",
    "    # Remove the symbols\n",
    "    text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]\n",
    "    # combine everything into one to make vocabs\n",
    "    word_list = list(set(\" \".join(text).split()))\n",
    "    word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  #special tokens.\n",
    "\n",
    "    #create the word2id\n",
    "    for i, w in enumerate(word_list):\n",
    "        word2id[w] = i + 4  #because 0-3 are already occupied\n",
    "    id2word = {i: w for i, w in enumerate(word2id)}\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "    #list of all tokens for whole text\n",
    "    token_list = list()\n",
    "    for sentence in sentences:\n",
    "        arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
    "        token_list.append(arr)\n",
    "    \n",
    "    return token_list, word2id, id2word, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised the make batch function\n",
    "def make_batch(sentence, batch_size, max_mask, max_len):\n",
    "    token_list,word2id,id2word,vocab_size = preprocess(\" \".join(sentence))\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size // 2 or negative != batch_size // 2:\n",
    "\n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = randrange(len(token_list)), randrange(len(token_list))\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3 masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]']\n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(datasets,max_mask=5,max_len=300):\n",
    "    # preprocess the dataset\n",
    "    sentences = [f\"{premise}{hypothesis}\"for premise,hypothesis in zip(datasets[\"premise\"],datasets[\"hypothesis\"])]\n",
    "    # make the batch\n",
    "    batch = make_batch(sentences,len(sentences),max_mask,max_len)\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([i[0] for i in batch]),\n",
    "        \"segment_ids\": torch.tensor([i[1] for i in batch]),\n",
    "        \"masked_tokens\": torch.tensor([i[2] for i in batch]),\n",
    "        \"masked_pos\": torch.tensor([i[3] for i in batch]),\n",
    "        \"labels\": torch.tensor([i[4] for i in batch], dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a90c952ab74fe284d40b02a7655dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866fab5a0d144396a7b59d2a24247970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf557bbf7a84936be4aaec0c85cc420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocessing the snli_dataset\n",
    "batch_size = 6\n",
    "tokenized_snli_dataset = snli_dataset.map(preprocess_datasets,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f587ea1f264875abae3049563c3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a7b476fae4422c8fd6b8a3c9b5966b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222599f787ec4e73acfc6b50f157df57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the tokenization of premise and hypothesis\n",
    "premise_dataset = tokenized_snli_dataset.map(lambda x: {'input_ids': x['input_ids'], \n",
    "                                                        'segment_ids': x['segment_ids'], \n",
    "                                                        'masked_tokens': x['masked_tokens'], \n",
    "                                                        'masked_pos': x['masked_pos'], \n",
    "                                                        'labels': x['labels']})\n",
    "hypothesis_dataset = tokenized_snli_dataset.map(lambda x: {'input_ids': x['input_ids'], \n",
    "                                                           'segment_ids': x['segment_ids'], \n",
    "                                                           'masked_tokens': x['masked_tokens'], \n",
    "                                                           'masked_pos': x['masked_pos'], \n",
    "                                                           'labels': x['labels']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove opposite dataset columns\n",
    "premise_dataset = premise_dataset.remove_columns(['hypothesis'])\n",
    "hypothesis_dataset = hypothesis_dataset.remove_columns(['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['hypothesis', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['hypothesis', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['hypothesis', 'label', 'input_ids', 'segment_ids', 'masked_tokens', 'masked_pos', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format -> Pytorch\n",
    "premise_dataset.set_format(\"torch\")\n",
    "hypothesis_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for primise dataset\n",
    "premise_loader_train = DataLoader(premise_dataset['train'],batch_size=batch_size,shuffle=True)\n",
    "premise_loader_test = DataLoader(premise_dataset['test'],batch_size=batch_size)\n",
    "premise_loader_valid = DataLoader(premise_dataset['validation'],batch_size=batch_size)\n",
    "# Data Loader for hypothesis dataset\n",
    "hypothesis_loader_train = DataLoader(hypothesis_dataset['train'],batch_size=batch_size,shuffle=True)\n",
    "hypothesis_loader_test = DataLoader(hypothesis_dataset['test'],batch_size=batch_size)\n",
    "hypothesis_loader_valid = DataLoader(hypothesis_dataset['validation'],batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4])\n",
      "torch.Size([4, 300])\n",
      "torch.Size([4, 300])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Pick one loader and check\n",
    "for batch in premise_loader_train:\n",
    "    print(len(batch['premise']))\n",
    "    print(batch['label'].shape)\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['segment_ids'].shape)\n",
    "    print(batch['masked_tokens'].shape)\n",
    "    print(batch['masked_pos'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4])\n",
      "torch.Size([4, 300])\n",
      "torch.Size([4, 300])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Pick one loader and check\n",
    "for batch in hypothesis_loader_train:\n",
    "    print(len(batch['hypothesis']))\n",
    "    print(batch['label'].shape)\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['segment_ids'].shape)\n",
    "    print(batch['masked_tokens'].shape)\n",
    "    print(batch['masked_pos'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S-BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(84, 768)\n",
       "    (pos_embed): Embedding(300, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=84, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = './model/bert.pth'\n",
    "loaded_model = BERT()\n",
    "loaded_model.load_state_dict(torch.load(load_path))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3)\n",
    "optimizer = torch.optim.Adam(loaded_model.parameters(), lr=2e-4)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "# warmup for the first ~10% steps\n",
    "total_steps = int(len(snli_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, classifier_head, premise_loader_train, hypothesis_loader_train, optimizer, optimizer_classifier, scheduler, scheduler_classifier, criterion):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "\n",
    "    for step, (premise_batch, hypothesis_batch) in enumerate(tqdm(zip(premise_loader_train, hypothesis_loader_train), leave=True, desc='Training: ')):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        # premise\n",
    "        input_ids_premise = premise_batch['input_ids']\n",
    "        segment_ids_premise = premise_batch['segment_ids']\n",
    "        masked_tokens_premise = premise_batch['masked_tokens']\n",
    "        masked_pos_premise = premise_batch['masked_pos']\n",
    "        labels_premise = premise_batch['labels']\n",
    "        # hypothesis\n",
    "        input_ids_hypothesis = hypothesis_batch['input_ids']\n",
    "        segment_ids_hypothesis = hypothesis_batch['segment_ids']\n",
    "        masked_tokens_hypothesis = hypothesis_batch['masked_tokens']\n",
    "        masked_pos_hypothesis = hypothesis_batch['masked_pos']\n",
    "        labels_hypothesis = hypothesis_batch['labels']\n",
    "        # Extract token embeddings from BERT at last_hidden_state\n",
    "        _,_,u = model(input_ids_premise, segment_ids_premise,masked_pos_premise)\n",
    "        _,_,v = model(input_ids_hypothesis, segment_ids_hypothesis,masked_pos_hypothesis)\n",
    "\n",
    "        # |u-v| tensor\n",
    "        uv = torch.sub(u, v)\n",
    "        uv_abs = torch.abs(uv)\n",
    "\n",
    "        # Concatenate u, v, |u-v|\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "\n",
    "        # Process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x)\n",
    "\n",
    "       # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, labels_premise)\n",
    "        \n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        epoch_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, classifier_head, premise_loader_valid,hypothesis_loader_valid, criterion):\n",
    "    epoch_loss = []\n",
    "    model.eval()\n",
    "    classifier_head.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (premise_batch, hypothesis_batch) in enumerate(tqdm(zip(premise_loader_valid, hypothesis_loader_valid), leave=True, desc='Evaluate: ')):\n",
    "            \n",
    "            # premise\n",
    "            input_ids_premise = premise_batch['input_ids']\n",
    "            segment_ids_premise = premise_batch['segment_ids']\n",
    "            masked_tokens_premise = premise_batch['masked_tokens']\n",
    "            masked_pos_premise = premise_batch['masked_pos']\n",
    "            labels_premise = premise_batch['labels']\n",
    "            # hypothesis\n",
    "            input_ids_hypothesis = hypothesis_batch['input_ids']\n",
    "            segment_ids_hypothesis = hypothesis_batch['segment_ids']\n",
    "            masked_tokens_hypothesis = hypothesis_batch['masked_tokens']\n",
    "            masked_pos_hypothesis = hypothesis_batch['masked_pos']\n",
    "            labels_hypothesis = hypothesis_batch['labels']\n",
    "            # Extract token embeddings from BERT at last_hidden_state\n",
    "            _,_,u = model(input_ids_premise, segment_ids_premise,masked_pos_premise)\n",
    "            _,_,v = model(input_ids_hypothesis, segment_ids_hypothesis,masked_pos_hypothesis)\n",
    "            \n",
    "           \n",
    "          \n",
    "            # build the |u-v| tensor\n",
    "            uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "            uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "            \n",
    "            # concatenate u, v, |u-v|\n",
    "            x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "            \n",
    "            # process concatenated tensor through classifier_head\n",
    "            x = classifier_head(x) #batch_size, classifer\n",
    "            \n",
    "            # calculate the 'softmax-loss' between predicted and true label\n",
    "            loss = criterion(x, labels_premise)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 1it [00:03,  3.67s/it]\n",
      "Evaluate: : 1it [00:01,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 4s\n",
      "\tTrain Loss: 1.109\n",
      "\t Val. Loss: 1.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 1it [00:04,  4.66s/it]\n",
      "Evaluate: : 1it [00:01,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 5s\n",
      "\tTrain Loss: 1.113\n",
      "\t Val. Loss: 1.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 1it [00:04,  4.53s/it]\n",
      "Evaluate: : 1it [00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 5s\n",
      "\tTrain Loss: 1.113\n",
      "\t Val. Loss: 1.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 1it [00:04,  4.86s/it]\n",
      "Evaluate: : 1it [00:01,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 5s\n",
      "\tTrain Loss: 1.113\n",
      "\t Val. Loss: 1.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 1it [00:04,  4.10s/it]\n",
      "Evaluate: : 1it [00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 5s\n",
      "\tTrain Loss: 1.107\n",
      "\t Val. Loss: 1.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 5\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(loaded_model, classifier_head, premise_loader_train, hypothesis_loader_train, optimizer, optimizer_classifier, scheduler, scheduler_classifier, criterion)\n",
    "    val_loss = evaluate(loaded_model, classifier_head, premise_loader_valid, hypothesis_loader_valid, criterion)\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # save the model only when its validation loss is lower than all its predecessors\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(classifier_head, './model/s_bert_classifier_head.pt')  # save the classifier head\n",
    "        torch.save(model.state_dict(), './model/s_bert.pth')  # save the model's parameters and state to a file\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Cosine Similarity: 0.5000\n"
     ]
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "with torch.no_grad():\n",
    "    for step, (premise_batch, hypothesis_batch) in enumerate(zip(premise_loader_valid, hypothesis_loader_valid)):\n",
    "         # premise\n",
    "            input_ids_premise = premise_batch['input_ids']\n",
    "            segment_ids_premise = premise_batch['segment_ids']\n",
    "            masked_tokens_premise = premise_batch['masked_tokens']\n",
    "            masked_pos_premise = premise_batch['masked_pos']\n",
    "            labels_premise = premise_batch['labels']\n",
    "            # hypothesis\n",
    "            input_ids_hypothesis = hypothesis_batch['input_ids']\n",
    "            segment_ids_hypothesis = hypothesis_batch['segment_ids']\n",
    "            masked_tokens_hypothesis = hypothesis_batch['masked_tokens']\n",
    "            masked_pos_hypothesis = hypothesis_batch['masked_pos']\n",
    "            labels_hypothesis = hypothesis_batch['labels']\n",
    "            # Extract token embeddings from BERT at last_hidden_state\n",
    "            _,_,u = loaded_model(input_ids_premise, segment_ids_premise,masked_pos_premise)\n",
    "            _,_,v = loaded_model(input_ids_hypothesis, segment_ids_hypothesis,masked_pos_hypothesis)\n",
    "\n",
    "    similarity_score = cosine_similarity(u.reshape(-1), v.reshape(-1))\n",
    "    total_similarity += similarity_score\n",
    "    \n",
    "average_similarity = total_similarity / (len(premise_loader_valid)+len(hypothesis_loader_valid))\n",
    "print(f\"Avg Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "hf_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want to encode. Example:\n",
    "sentences = [\"The cat sat on the mat by the door.\", \"The dog walk on the mat by the door.\"]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.6528433561325073\n"
     ]
    }
   ],
   "source": [
    "# Hugging face model\n",
    "u_hug = torch.tensor(embeddings[0], dtype=torch.float32)\n",
    "v_hug = torch.tensor(embeddings[1], dtype=torch.float32)\n",
    "\n",
    "# cosine similarity\n",
    "cos_sim = cosine_similarity(u_hug.reshape(1, -1), v_hug.reshape(1, -1))[0, 0]\n",
    "print(f'Cosine similarity: {cos_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torchtext\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def inputs(sentence, tokenizer, vocab, max_len):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = tokenizer(re.sub(\"[.,!?\\\\-]\", '', sentence.lower()))\n",
    "    \n",
    "    # Filter tokens that are not in the vocabulary\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    \n",
    "    # Add special tokens ([CLS] and [SEP]) and convert tokens to their corresponding IDs\n",
    "    input_ids = [vocab['[CLS]']] + [vocab[token] for token in tokens] + [vocab['[SEP]']]\n",
    "    \n",
    "    # Pad input_ids with 0s if its length is less than max_len\n",
    "    n_pad = max_len - len(input_ids)\n",
    "    input_ids += [0] * n_pad\n",
    "    \n",
    "    # Generate attention mask\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    return {'input_ids': torch.LongTensor(input_ids).reshape(1, -1),\n",
    "            'attention_mask': torch.LongTensor(attention_mask).reshape(1, -1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inputs() missing 2 required positional arguments: 'word2id' and 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[250], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love cat.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: inputs() missing 2 required positional arguments: 'word2id' and 'max_len'"
     ]
    }
   ],
   "source": [
    "inputs(\"I love cat.\", tokenizer, word2id, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/word2id.pickle','rb') as f:\n",
    "  word2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, vocab, sentence_a, sentence_b):\n",
    "    # Get input IDs and segment IDs for sentence_a\n",
    "    inputs_a = get_inputs(sentence_a, tokenizer, vocab, max_len)\n",
    "    input_ids_a = torch.tensor(inputs_a['input_ids']).unsqueeze(0)  # Add batch dimension\n",
    "    segment_ids = torch.tensor([0] * len(input_ids_a))  # Assuming single segment\n",
    "    \n",
    "    # Get input IDs for sentence_b\n",
    "    inputs_b = get_inputs(sentence_b, tokenizer, vocab, max_len)\n",
    "    input_ids_b = torch.tensor(inputs_b['input_ids']).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    print(input_ids_a.shape,input_ids_b.shape)\n",
    "    # Get token embeddings for sentence_a and sentence_b\n",
    "    u = model.get_last_hidden_state(input_ids_a, segment_ids)\n",
    "    v = model.get_last_hidden_state(input_ids_b, segment_ids)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.detach().numpy().reshape(1, -1), v.detach().numpy().reshape(1, -1))[0, 0]\n",
    "    \n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 300]) torch.Size([1, 1, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Local\\Temp\\ipykernel_14320\\642347818.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_a = torch.tensor(inputs_a['input_ids']).unsqueeze(0)  # Add batch dimension\n",
      "C:\\Users\\earth\\AppData\\Local\\Temp\\ipykernel_14320\\642347818.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_b = torch.tensor(inputs_b['input_ids']).unsqueeze(0)  # Add batch dimension\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[247], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sentence_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour contribution helped make it possible for us to provide our students with a quality education.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m sentence_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour contributions were of no help with our students\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m education.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine Similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[246], line 17\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(model, tokenizer, vocab, sentence_a, sentence_b)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids_a\u001b[38;5;241m.\u001b[39mshape,input_ids_b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get token embeddings for sentence_a and sentence_b\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_last_hidden_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m v \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_last_hidden_state(input_ids_b, segment_ids)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 42\u001b[0m, in \u001b[0;36mBERT.get_last_hidden_state\u001b[1;34m(self, input_ids, segment_ids)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_last_hidden_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, segment_ids):\n\u001b[0;32m     41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_ids, segment_ids)\n\u001b[1;32m---> 42\u001b[0m     enc_self_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_attn_pad_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     44\u001b[0m         output, enc_self_attn \u001b[38;5;241m=\u001b[39m layer(output, enc_self_attn_mask)\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36mget_attn_pad_mask\u001b[1;34m(seq_q, seq_k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_attn_pad_mask\u001b[39m(seq_q, seq_k):\n\u001b[1;32m----> 2\u001b[0m     batch_size, len_q \u001b[38;5;241m=\u001b[39m seq_q\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m      3\u001b[0m     batch_size, len_k \u001b[38;5;241m=\u001b[39m seq_k\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# eq(zero) is PAD token\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(loaded_model, tokenizer, word2id, sentence_a, sentence_b)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
